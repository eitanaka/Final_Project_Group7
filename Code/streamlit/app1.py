"""
Author: Ei Tanaka, Cody Yu, Pon Swarnalaya Ravichandran
Date: December 11, 2023
Purpose: This file is used to run the streamlit app
References:
    https://github.com/blackary/st_pages?tab=readme-ov-file (streamlit pages)
"""

# ============================== imorts =======================================
import os
from tkinter import Image
from PIL import Image
import streamlit as st
from st_pages import Page, show_pages, add_page_title


# ============================== Setup / constants ====================================
# Set up paths
OS_PATH = os.getcwd()
os.chdir("../../")
ROOT_PATH = os.getcwd()
IMAGE_PATH = os.path.join(OS_PATH, 'assets')
MODEL_PATH = os.path.join(ROOT_PATH, "Models")
ELECTRA_PATH = os.path.join(MODEL_PATH, "ELECTRA", "electra-finetuned-squadv2")
XLNET_PATH = ""
os.chdir(OS_PATH)

# Model Checkpoints
electra_checkpoint = "google/electra-base-discriminator"
electra_finetuned_checkpoint = "checkpoint-49410"
XLNET_checkpoint = ""
XLNET_finetuned_checkpoint = ""

# Specify what pages should be shown in the sidebar, and what their labels should be
show_pages(
    [
        Page("app.py", "Introduction"),
        Page("app_EDA.py", "Exploratory Data Analysis"),
        Page("app_Model.py", "Model"),
        Page("app_Results.py", "Results"),
        Page("app_demo.py", "Demo"),
    ]
)

# =============.================= Main Contents ====================================
st.title("Question Answering on SQuAD 2.0")

st.header("Introduction")

st.write("In the dynamic realm of Natural Language Processing (NLP), the advent of question-answering (QA) systems marks a significant stride in our ability to interact with and process digital information. "
         "This project is dedicated to developing a reading comprehension-based QA system inspired by the comprehensive insights in Speech and Language Processing by Daniel Jurafsky & James H. Martin (2023). "
         "We aim to create a system that can interpret and respond to questions posed in natural language, drawing answers from provided text passages.")

st.header("NLP Task - Question Answering")

st.write("In the Natural Language Processing (NLP) field, extractive Question Answering (QA) is a pivotal task involving locating the answer to a question within a specified text passage. "
         "This task is inherently challenging, as it requires the system to comprehend the posed question and accurately extract the specific text portion that contains the answer. "
         "As detailed in Hugging Face's documentation and task library (n.d.), "
         "extractive QA demands the capability to sift through extensive text and pinpoint information that precisely responds to the query.")

IMAGE_PATH2 = os.path.join(IMAGE_PATH, "Figure_nlp_task.png")
st.write("Figure 01")
img2 = Image.open(IMAGE_PATH2)
st.image(img2)

st.write("Jurafsky and Martin (2023), in their seminal work," "Speech and Language Processing," "elucidate the complexities of extractive QA, highlighting the necessity for advanced NLP techniques and models."
"These models are crucial for understanding the context and semantics embedded in both the question and the passage, thus enabling the identification of the exact text span that answers the question."
"Extractive QA is particularly vital in scenarios necessitating factual answers directly sourced from the provided text, such as in academic research or specific information retrieval tasks."
"In our project, we embrace the challenges of extractive QA by training our model on the SQuAD 2.0 dataset. "
"This dataset, encompassing diverse questions and passages, provides a comprehensive framework for the system to learn from varying contexts and question types." ""
"The model is meticulously trained to parse the subtleties of language in questions and passages, enhancing its ability to discern and extract the relevant answers accurately." 
"This endeavor underscores the significance of sophisticated text processing and comprehension in NLP, laying the groundwork for more intelligent and adept information retrieval systems.")

st.header("Dataset")

st.write("SQuAD 2.0 stands as an innovative dataset strategically crafted to propel advancements in the domain of machine reading comprehension—an integral facet of natural language understanding. "
         "Distinguished from its predecessor, SQuAD 1.1, this dataset introduces a notable enhancement: the integration of 53,775 unanswerable questions meticulously generated by crowdworkers. "
         "These questions leverage the same passages employed in SQuAD 1.1 but are intentionally formulated to maintain relevance to the content while introducing plausible yet incorrect answers. "
         "This deliberate shift from the preceding version, which solely featured answerable questions with correct responses within the provided text, signifies a significant evolution. "
         "Below, we present two illustrative examples of this distinctive design.")

IMAGE_PATH1 = os.path.join(IMAGE_PATH, "FIGURE01.png")
st.write("Figure 02")
img = Image.open(IMAGE_PATH1)
st.image(img)
st.write("The principal objective underlying the conception of SQuAD 2.0 is to present a formidable challenge to and elevate the proficiency of machine learning models in discerning instances where a correct answer is absent within the provided text." 
         "This task introduces a heightened level of complexity compared to SQuAD 1.1, wherein models were tasked with identifying the text span most pertinent to the posed question." 
         "SQuAD 2.0 aspires to cultivate a deeper understanding and critical analysis, thereby pushing the boundaries of machine learning models' capabilities in comprehending and interpreting textual information."
        "The dataset has undergone meticulous testing, affirming its status as both a challenging and high-quality resource."
        "Evaluation with cutting-edge machine learning models reveals a markedly lower F1 score of 66.3% for SQuAD 2.0, contrasting with the human benchmark of 89.5% F1 score."
        "This discrepancy underscores the heightened difficulty of the dataset, as evidenced by the same model architecture achieving an 85.8% F1 score on SQuAD 1.1—reflecting a performance closer to human levels." 
         "Notably, the unanswerable questions within SQuAD 2.0 have demonstrated greater difficulty compared to those generated by alternative methods such as distant supervision or rule-based approaches.")

st.write("SQuAD 2.0 has been released to the public and currently stands as the primary benchmark on the official SQuAD leaderboard. The introduction of this dataset represents a significant stride in the advancement of sophisticated reading comprehension systems."
        "It is anticipated to catalyze the development of models with an enhanced ability to comprehend text and discern the limits of their knowledge, a fundamental aspect of effective language comprehension."
        "During the creation of the SQuAD 2.0 dataset, crowdworkers sourced from the Daemo crowdsourcing platform were engaged to formulate unanswerable questions. Their task involved generating up to five questions per paragraph, extracted from articles in SQuAD 1.1, "
        "with the explicit condition that these questions could not be answered based on the paragraph alone. "
        "Despite this constraint, the questions were required to feature plausible answers and reference entities within the paragraph. To guide the process, workers were presented with corresponding questions from SQuAD 1.1 and encouraged to structure the unanswerable questions to resemble the answerable ones."
         " Each worker dedicated approximately 7 minutes to each paragraph and received compensation of $10.50 per hour."
        "To uphold quality standards, questions from workers who produced fewer than 25 questions per article were excluded. This approach was instrumental in filtering out contributions from individuals facing challenges with the assigned task. "
         "Subsequently, the dataset underwent partitioning into training, development, and test sets, adhering to the same article distribution as SQuAD 1.1, amalgamating new and existing data. In the development and test sets, articles lacking unanswerable questions were omitted, ensuring an approximately equal ratio of answerable and unanswerable questions."
         " However, the training data exhibited about twice as many answerable questions as unanswerable ones.")

st.write("In creating the SQuAD 2.0 dataset, crowdworkers from the Daemo crowdsourcing platform were employed to generate unanswerable questions." ""
         "Their task involved creating up to five questions per paragraph from articles in SQuAD 1.1, stipulating that these questions could not be answered based on the paragraph alone. " 
         "Despite this, the questions had to contain plausible answers and reference entities within the paragraph." ""
         "To guide the process, workers were shown corresponding questions from SQuAD 1.1 and encouraged to make the unanswerable questions resemble the answerable ones." 
         "Each worker dedicated approximately 7 minutes to each paragraph and received $10.50 per hour compensation.")

st.write("Questions from workers who wrote fewer than 25 questions per article were discarded to maintain quality." 
         "This approach helped filter out contributions from those who struggled with the task. The dataset was then divided into training, development, and test splits, following the same article partition as SQuAD 1.1, combining new and existing data." 
         "In the development and test sets, articles without unanswerable questions were removed, leading to an approximately equal ratio of answerable and unanswerable questions. However, The training data had about twice as many answerable questions as unanswerable ones")

IMAGE_PATH3 = os.path.join(IMAGE_PATH, "Fig_03.png")
st.write("Figure 03")
img3 = Image.open(IMAGE_PATH3)
st.image(img3)
st.write('The instructions are shown to crowdworkers at the beginning of each question writing task. (Rajpurakar, et al. 2018)' )

st.write("For dataset validation, additional crowdworkers were hired to answer all questions in the development and test sets of SQuAD 2.0." 
         "They were presented with entire articles and associated questions, which were a mix of answerable and unanswerable types." ""
         "Workers had to highlight the answer in the paragraph or mark the question as unanswerable, with an average time of one minute spent per question." 
         "Multiple answers were collected for each question to ensure accuracy and reduce response variance. The final answer was determined through a majority vote, with a preference for shorter answers and a tendency to resolve ties by opting to provide an answer." 
         "On average, 4.8 responses were gathered per question. This method contrasts with the evaluation of SQuAD 1.1, where only a single human's performance was assessed, likely leading to an underestimation of human accuracy in the earlier dataset.")
IMAGE_PATH3 = os.path.join(IMAGE_PATH, "Fig 04.png")
st.write("Figure 04")
img4 = Image.open(IMAGE_PATH3)
st.image(img4)

